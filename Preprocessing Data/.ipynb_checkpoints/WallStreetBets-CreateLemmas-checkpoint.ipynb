{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5HrGvjvj5AZ"
   },
   "source": [
    "### In this session we Lemmatization the csv file for doing this we do the following step\n",
    "#### 1. Import libraries\n",
    "#### 2. Build Pipeline\n",
    "#### 3. output CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sltLjq_5i-Z_"
   },
   "source": [
    "## Installing Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36746,
     "status": "ok",
     "timestamp": 1618624539736,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "tCK3YmBuXKSV",
    "outputId": "8cf103e5-f9e0-4168-d659-fc26807babc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/soyanswartz/.local/lib/python3.10/site-packages (1.23.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/soyanswartz/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (4.37.3)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (1.23.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/soyanswartz/.local/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/soyanswartz/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/soyanswartz/.local/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/soyanswartz/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/soyanswartz/.local/lib/python3.10/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/soyanswartz/.local/lib/python3.10/site-packages (from scikit-learn) (1.23.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/soyanswartz/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "--2022-10-28 14:12:55--  http://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
      "--2022-10-28 14:12:55--  https://setup.johnsnowlabs.com/colab.sh\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2022-10-28 14:12:55--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1191 (1.2K) [text/plain]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-10-28 14:12:56 (21.9 MB/s) - written to stdout [1191/1191]\n",
      "\n",
      "Installing PySpark 3.2.1 and Spark NLP 4.2.2\n",
      "setup Colab for PySpark 3.2.1 and Spark NLP 4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
    "!pip install sparknlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark NLP session object\n",
    "### **Note:** first time you may get error after installing the package for starting sparknlp but second time run it again you should be find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1618624719432,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "mSdK4WiaZJoO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 14:48:48 WARN Utils: Your hostname, soyanswartz-Blade resolves to a loopback address: 127.0.1.1; using 192.168.1.183 instead (on interface wlp3s0)\n",
      "22/10/28 14:48:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/soyanswartz/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/soyanswartz/.ivy2/cache\n",
      "The jars for the packages stored in: /home/soyanswartz/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1c83661c-90e0-43ad-b1e2-a584a6b9006d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 411ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1c83661c-90e0-43ad-b1e2-a584a6b9006d\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 16 already retrieved (17448kB/36ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 14:48:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6tE0twHjnwa"
   },
   "source": [
    "### Read data from my ( 100days.csv ) file\n",
    "\n",
    "* **first we remove all this data that is not have any growth**\n",
    "\n",
    "* **second we take 1 for this row that has growth above 6% and take 0 for this row that have growth below 6%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1618624872032,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "jrMOC8X5Zk5p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Give a class label to each post. 1 for >= 6% growth, otherwise 0.\n",
    "'''\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "data = spark.read.csv(\"100days.csv\", header=True)\n",
    "#Filtering out data without a growth\n",
    "data = data.filter(data.growth != \"N/A\")\n",
    "#Labeling data, 1 if growth % is >= 6 otherwise 0.\n",
    "function = udf(lambda growth: 1 if float(growth[:-1]) >=6 else 0, IntegerType())\n",
    "data = data.withColumn('label', function(data.growth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1618624722160,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "Vi9i8O2RlaDF"
   },
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp import Finisher\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing other column and just use this three column\n",
    "    1. id\n",
    "    2. label\n",
    "    3. text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3919,
     "status": "ok",
     "timestamp": 1618624727070,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "d0GUtW7eaVbD",
    "outputId": "f407a24d-224b-43f9-bea5-9e9bda9efd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "finisher = Finisher().setInputCols([\"token\", \"lemmas\", \"pos\"])\n",
    "explain_pipeline_model = PretrainedPipeline(\"explain_document_ml\").model\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        explain_pipeline_model,\n",
    "        finisher\n",
    "        ])\n",
    "\n",
    "model = pipeline.fit(data.select('id', 'label', 'text'))\n",
    "annotations_finished_df = model.transform(data.select('id','label', 'text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing this post that length is below 10 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1618624727969,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "mCN7ZcwTji1h"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "text_lemmas = annotations_finished_df.select('id', 'label', 'finished_lemmas')\n",
    "# remove posts for which the text is too short\n",
    "function_length = udf(lambda r: len(r) > 10, BooleanType())\n",
    "text_lemmas = text_lemmas.withColumn('word_count', function_length('finished_lemmas')).filter(col('word_count')).drop('word_count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split each word of text by ( | (Pipe) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 50885,
     "status": "ok",
     "timestamp": 1618624782523,
     "user": {
      "displayName": "Sispapjen",
      "photoUrl": "",
      "userId": "00407414900530577219"
     },
     "user_tz": 240
    },
    "id": "_CpamzpdkAOX"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make lemmas writable to CSV\n",
    "'''\n",
    "\n",
    "output = text_lemmas.rdd.map(lambda row: row(id=row['id'], label=row['label'], text='|'.join(row['finished_lemmas'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the change to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 15:42:48 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 843867 ms exceeds timeout 120000 ms\n",
      "22/10/28 15:42:49 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "output_df.coalesce(1).write.csv('Data/lemma-100days-wsbdata.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WallStreetBets-CreateLemmas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
